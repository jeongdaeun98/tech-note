# Kafka
### Kafka 왜 썼는가, 장단점은, 역사는?
1. 파이프라인의 파편화로 관리하기 어려웠고,
2. 서로 연관되어있어 특정 부분을 수정하거나 확장시킬 때도 연관된 부분을 모조리 찾아 수정해야했습니다. 

#### 따라서 링크드인에서는 사내 모든 데이터들과 이벤트의 흐름을 중앙에서 처리하는 시스템을 만들었고 그것이 카프카였습니다.


![](https://i.imgur.com/MiajoZ0.jpg)
### 카프카 장점은 무엇인지?
카프카를 사용하면서 좋았던 것은 온라인 상에서 브로커를 추가시킬 수 있고 프로듀서와 컨슈머가 분리되어있어 다른 영역을 수정할 필요 없이 스케일 아웃이 쉽다는 점이다.

### 단점은 무엇인지?
클러스터 내 브로커 내부에 다중 파티션이라고 가정했을 때,
파티션 내에서는 순서가 보장되나 파티션끼리는 순서가 보장되지 않는다는 점이 있다.
그리하여 단일 파티션으로 관리해야하는데 
이 경우 카프카의 장점인 대용량 데이터에 대한 부하를 처리하기 위한 스케일 아웃을 하지 못한다는 것이다.
스케일아웃은 부하처리를 위해서는 필연적이기 때문에
카프카는 순서가 보장될 필요가 없는 데이터일 경우 적합하다.

### 처리량을 높이기 위한 카프카의 노력
- 페이지 캐시
    한번 읽은 파일 내용을 페이지 캐시 영역에 저장한다.
    같은 파일에 대한 접근이 일어나면 디스크가 아닌 페이지의 캐시에서 읽어온다.
- 배치 전송 처리
    프로듀서는 메시지를 묶어서 한번에 보내어서 오버헤드를 줄인다.

### 토픽과 파티션
토픽은 카프카에서 메시지를 논리적인 형태로 분리해놓은 것이고
파티션은 실제로 메시지가 저장되는 물리적인 저장소이다.
토픽을 나눈 것이 파티션이다.

## kafka producer
### 역할
토픽으로 메시지를 전송하는 역할을 담당하고 있다. 
이러한 쓰기 요청은 모두 파티션의 리더 리플리카에게 전송되어야한다.
따라서 메타 데이터 요청을 통해 관심갖는 토픽 정보를 알 수 있다.
어떤 리플리카가 리더인지에 대한

### 내부적으로 실행되는 과정
1. 우선 카프카에 쓰려는 메시지를 갖는 producerRecord를 생성합니다. ProducerRecord는 개발자가 전송하기 원하는 토픽과 값을 포함하며, 선택적으로 키와 파티션을 지정할 수도 있습니다. 
2. 그리고 직렬처리기 컴포넌트에 전달된 후, 객체가 네트워크로 전송될 수 있도록 바이트 배열로 직렬화합니다.
3. 그 다음에 그 데이터는 파티셔너 컴포넌트에 전달됩니다. 만일 producerRecord에 특정 파티션을 지정했다면 파티셔너는 어떤 처리 없이 지정된 파티션을 반환합니다. 그러나 파티션을 설정하지 않았다면 producerRecord의 키를 기준으로 파티셔너가 하나의 파티션을 선택해줍니다.
4. 파티션이 선택되면 해당 레코드의 메시지가 저장될 토픽과 파티션을 프로듀서가 알게됩니다.
 그 다음 같은 토픽과 파티션으로 전송될 레코드들을 모은 레코드 배치에 추가하여 별개의 스레드가
그 배치를 카프카 브로커에게 전송합니다.
5. 브로커는 수신된 레코드의 메시지를 처리한 후 응답을 전송합니다. 이때 메시지를 성공적으로 쓰면 RecordMetadata 객체를 반환합니다. 이 객체는 토픽, 파티션, 파티션 내부의 메시지 오프셋을 갖습니다. 그러나 메시지 쓰기에 실패하면 에러를 반환하며, 에러를 수신한 프로듀서는 메시지 쓰기를 포기하고 에러를 반환하기 전에 몇번 더 재전송합니다. 
카프카 프로듀서가 자동으로 재 전송해주므로 애플리케이션에는 재전송을 처리하는 코드가 필요없습니다. 
>따라서 재전송이 불가능한 에러거나 재전송 횟수만큼 재전송되었을 때 처리할 코드만 작성하면 됩니다.
### Kafka producer에서 메시지 손실을 줄이기 위해 ack config를 설정했을 때 
1. 메시지 손실 가능성 👍🏻, 전송 속도 👍🏻 : ack 0
> 메시지 잘 받았는지 못받았는지 여부를 따지지 않는다.
이 경우 평소에는 메시지를 잘 받다가 브로커가 다운 되는 장애가 일어날 때 손실 가능성이 매우 높다.
2. 메시지 손실 가능성 👎🏻, 적당한 속도: ack 1
> leader가 메시지를 잘 받았는지에 대한 ack를 전송한다.
follower가 leader 메시지를 가져오기 전에 장애가 발생할 경우 메시지 손실할 가능성이 있다.
3. 메시지 손실 ❌, 속도 👎🏻 : ack all or -1, 브로커의 min.insync.replicas
 > 리더가  min.insync.replicas 이 옵션에 따라 n개의 follower가 잘 replication 했는지를 검사한 뒤 ack를 보낸다.
그러나 너무 높은 수로 지정했을 경우, 브로커가 하나만 다운되어 리더를 선출할 수 있는 상황에서도 옵션에 대한 조건을 충족시킬 수 없는 상황이 발생하였기 때문에 카프카로 메시지를 보낼 수 없는 클러스터 전체 장애와 비슷한 상황이 발생한다.

### 파티션을 신중하게 설정해야하는 이유
 파티션에 저장된 데이터 파일은 2개로 원본 데이터와 인덱스 데이터인데,
 1. 파티션내에 모든 파일 핸들러를 열어야하므로 **리소스 낭비의 문제**가 있을 수 있습니다.
 2. 파티션이 많을 수록 **복구 시간이 증가합니다**.
상황을 가정해봅시다. 첫번째로 카프카 브로커가 죽었을 때, 이 브로커가 1000개의 파티션에 대한 리더라고 가정합니다. 그렇다면 리더를 새로 선출할 때까지 데이터를 읽을 수도 쓸수도 없기에 새로운 리더를 선출해야합니다.
만약 파티션 하나에 대해 리더를 선출하는 시간이 5밀리초라고 가정했다면, 장애 복구 시간은 5초가 걸립니다.
### 파티션 개수를 선택할 때 유의할 점
- 토픽의 목표 처리량을 컨슈머 예상 처리량으로 나누는 방법으로 파티션 수를 계산할 수 있습니다. 예를 들어 초당 1GB로 토픽을 읽고 쓰길 원하는데 한 컨슈머가 초당 50MB만 처리할 수 있다면 최소한 20개의 파티션이 필요하다는 것을 알 수 있습니다.

## kafka consumer
### 역할?
카프카 브로커의 토픽을 컨슘하여 데이터를 가져오는 역할을 담당한다.
### 카프카 컨슈머가 데이터를 읽어오는 과정
1. 컨슈머는 하나이상의 토픽을 구독한다. 
2. 컨슈머는 브로커로부터 연속적으로 많은 데이터를 읽기 위해 폴 루프에 있다. 폴루프에서 지속적으로 폴하면서 하트비트를 보낸다. 그렇지 않으면 코디네이터는 해당 컨슈머가 중단된 것으로 간주하고 리밸런싱을 시작한다. 자동커밋으로 설정되어있을 경우, 폴링할 때 설정된 시간이 지나면 마지막으로 호출된 poll 메서드에서 반환된 오프셋을 커밋한다. 즉 카프카에 offsetCommit 요청이 전송된다.
3. 새로운 레코드가 있으면 가져온다.
4. 역직렬화한다. 
### offset이란?
파티션안에 데이터 위치를 유니크한 숫자로 표시한 것이다. 컨슈머는 상황에 따라 offset을 변경할 수 있다.
### 컨슈머 그룹이 뭔지?
다수의 컨슈머 인스턴스로 이루어진 논리적인 그룹이다. 첫번째로는 가용성을 위해 구성되었다. 두번째는 컨슈머 그룹형태로 분리하면서 그룹 별로 파티션의 오프셋을 관리하여 메시지를 읽기 위함이다. 이는 메시지를 디스크에 지속적으로 보장하는 이유와도 연관성이 있다. 디스크에 일정 기간동안 메시지를 보장한다면 publish 횟수에 관계없이 많은 컨슈머그룹들이 메시지를 읽을 수 있기 때문이다.
### 카프카 컨슈머 그룹에서 scale out 시키는 이유 - 처리속도와 가용성때문에
- 컨슈머 그룹에서 컨슈머가 한개일 경우 프로듀서에서 메시지를 전송하는 속도보다 컨슘하는 속도가 떨어질 수 있다. db에 데이터를 저장하는 작업이나 시간이 오래 걸리는 연산을 하는 경우 컨슘 속도가 떨어지는데, 이 경우 메시지를 늦게 가져올 수 있다. 따라서 이런 경우 파티션개수와 컨슈머 개수를 늘려 메시지 처리율을 높이는 데 중요하다. 
- 또한 컨슈머 인스턴스가 하나인데 장애가 발생할 경우 장애를 해결할 때까지 더이상 데이터를 처리하지 못해 운영 상에 문제가 생긴다. 따라서 가용성을 위해서도 scale out 시켜야한다.
### 파티션 하나당 컨슘할 수 있는 컨슈머 개수가 하나인 이유
컨슈머는 카프카 특성상 pub sub모델이라는 느슨한 구조로써 프로듀서를 알 수 없으며 그렇기에 프로듀서가 어떤 순서로 메시지를 전송한지 알 수 없다. 파티션 내부에 오프셋을 순서대로 읽음으로써 순서를 보장할 수 밖에 없다. 따라서 파티션 내에서는 순서 보장이 되지만 파티션 외에서는 순서 보장을 할 수 없다. 그러나 파티션 하나당 컨슈머 두개가 매핑되어서 컨슘한다고 가정할 경우,
 a가 파티션의 데이터를 읽고 오프셋을 커밋한다. 그 후 b는 그 다음 오프셋에 해당하는 데이터를 읽는다. 이 경우 어떤 컨슈머가 우선이 되는 데이터를 읽었는지를 알 수 없을 것이다. 그렇다면 파티션과 오프셋 정보를 가져와야할텐데 복잡해진다.
또한 a 컨슈머가 이미 데이터를 읽고 오프셋을 커밋하기 전에 b 또한 그 데이터를 읽어버려서 중복적인 데이터를 읽을 가능성이 있다.
따라서 파티션 하나당 컨슈머 하나가 매핑되어야한다.
### 카프카 컨슈머에서 파티션간에 메시지 순서 보장이 안되는 이유
컨슈머는 카프카 특성상 pub sub모델이라는 느슨한 구조로써 프로듀서를 알 수 없으며 그렇기에 프로듀서가 어떤 순서로 메시지를 전송한지 알 수 없다. 파티션 내부에 오프셋을 순서대로 읽음으로써 순서를 보장할 수 밖에 없다. 따라서 파티션 내에서는 순서 보장이 되지만 파티션 외에서는 순서 보장을 할 수 없다.
### 카프카 컨슈머에서의 리밸런싱
- 파티션의 소유권이 변경되는 것을 리밸런싱이라고 한다.
- 카프카 컨슈머에서는 파티션 소유권을 유지하기 위해 지속적으로 하트 비트를 전송한다.
컨슈머가 폴링할 때 혹은 읽은 메시지를 커밋할 때 하트비트는 자동 전송된다.(폴링은 메시지를 읽는 것을 말하며 주로 무한루프에서 연속적으로 수행시킨다.)
- 만일 컨슈머가 세션 시간을 넘길 때까지 하트비트를 전송하는 것을 중단하게 되거나, 혹은 컨슈머에 문제가 생기게 된다면 리밸런싱이 일어나게 된다. 리밸런싱이 일어나게 되면 그 컨슈머 그룹에 속한 모든 컨슈머들은 파티션의 데이터를 가져올 수 없게 된다.
### 파티션 할당 처리 절차
컨슈머가 그룹에 합류하고 싶을 땐 그룹코디네이터에게 joingroup 요청을 전송하면 된다.
이때 그룹에 **첫번째로 합류하는 컨슈머**는 **그룹 리더**가 된다.
리더는 그룹 코디네이터로부터 해당 그룹에서 살아있다고 간주되는 모든 컨슈머 내역을 받을 수 있으며, 그 그룹의 각 컨슈머들에게 파티션을 할당하는 책임을 갖는다.
그리고 이때 partitionAssignor 클래스를 사용하여 어떤 파티션들을 어느 컨슈머가 처리할 것인지 결정한다.
컨슈머들에게 할당할 파티션이 결정된 후 컨슈머 그룹 리더는 할당 내역을 그룹 코디네이터에게 전송한다.
각 컨슈머는 자신의 할당 내역만 알 수 있다.
이 절차는 파티션의 리밸런싱이 생길 때마다 반복 수행된다.
### 파티션 개수보다 컨슈머 개수가 적을 때 문제점
- 만일 파티션의 개수보다 컨슈머의 개수가 적은 경우 파티션 여러개에 대해 하나의 컨슈머가 감당해야하는데 이 경우엔 다른 컨슈머와 비교했을 때 부하가 크고 다운될 가능성이 있다. 따라서 다운이 될 경우 리밸런싱을 통해 다른 컨슈머가 파티션을 여러개 소유하도록 변경할 수 있다.
- 그러나 이런 상황을 지속하는 것은 좋은 방식이 아니다. 왜냐하면 지속적으로 처리가 되지 않은 메시지들이 쌓이게 되면서 메시지 생성 시간과 메시지 처리 시간에 차이가 더욱 커지게 되기 때문이다.

## kafka broker
### 어떻게 카프카 클라이언트가 토픽에 대한 정보를 알 수 있는 것인지?
![](https://i.imgur.com/l9Jen7M.jpg)

브로커에 메타데이터 요청을 통해 정보를 알 수 있다. 그 정보에는 토픽에 존재하는 파티션들, 각 파티션의 리플리카, 어떤 리플리카가 리더인지에 대한 정보를 포함한다.
클라이언트는 대부분 그 정보를 캐시에 보존 후 토픽에 해당하는 파티션 리더에게 쓰기 혹은 읽기 요청을 전송한다. 또한 가끔 그런 정보를 새로 교체해야한다. 교체 주기는 config로 제어한다.
> 메타데이터 요청은 모든 브로커에 전송해도 상관없다. 모든 브로커가 그 정보를 포함하는 메타데이터 캐시를 갖고 있기 때문이다.
### 컨트롤러 역할?
파티션 리더를 선출하는 책임을 갖는다.
### 컨트롤러가 리더 선출하는 과정?
1. 주키퍼를 watch하여 특정 브로커가 클러스터를 떠났음을 인지하면 그 브로커가 리더로 할당되었던 모든 파티션들에 새로운 리더가 필요하다는 것을 알게 된다.
2. 그 다음 컨트롤러는 새로운 리더를 필요로 하는 모든 파티션들을 점검하고 새로 리더가 될 브로커를 결정한다.
3. 특정 파티션이 새로운 리더를 갖는다는 것을 알리기 위해 새 리더와 팔로어들에게 leaderAndIsr 요청을 전송한다. 클라이언트 요청을 받아야한다는 것을 새 리더에게 알리고 팔로어들은 새 리더를 복제해야한다는 것을 알아야하기 때문이다.
### 컨트롤러 선출하는 과정?
1. 컨트롤러 브로커가 중단되거나 주키퍼와의 연결이 끊어지면 컨트롤러 노드가 삭제된다.
2. 해당 클러스터의 다른 브로커들이 주키퍼 watch를 통해 그를 알게 되고 컨트롤러 노드 생성을 시도한다.
3. 그 노드를 첫번째로 생성한 브로커가 컨트롤러가 된다. 컨트롤러는 매번 새로 선출될 때마다 주키퍼로부터 새로운 컨트롤러 세대 번호를 받으며 따라서 변경 전의 컨트롤러와 혼동되지 않는다. 만일 이전 세대번호로 된 컨트롤러 메시지를 받으면 무시한다.
### 메타 데이터 캐시를 교체하지 않을 경우
파티션 리더가 아니라는 에러를 받게 될텐데 그런 에러를 받게 되면 또다시 그 파티션에게 요청을 하지 않고 메타데이터 요청을 한다.
메타데이터 캐시가 교체되지 않아 오래된 정보를 사용하여 엉뚱한 브로커에게 요청을 전송한 것이기 때문이다.
### 카프카 브로커의 역할?
카프카 브로커는 프로듀서나 컨슈머 클라이언트의 요청을 처리하고 응답한다. 모든 요청은 항상 수신된 순서로 처리된다. 따라서 저장되는 메시지의 순서가 보장된다.
### 카프카 브로커 내부 요청 흐름?
![](https://i.imgur.com/xnRFPMa.jpg)

브로커는 자신이 리스닝 하는 각 포트에 대해서 acceptor 스레드를 실행하며 이 스레드는 연결을 생성하고 네트워크 스레드가 그 다음을 처리하도록 넘겨준다.
네트워크 스레드는 클라이언트 연결로부터 요청을 받고 그것을 요청큐에 넣으며 응답큐로부터 응답을 가져와서 클라이언트에게 전송하는 일을 수행한다.
일단 요청이 요청 큐에 위치하면 입출력 스레드가 각 요청을 가져와서 처리하는 책임을 갖는다.
### 카프카 브로커가 쓰기 요청을 받았을 때 흐름?
 > acks 매개변수에 따라 메시지를 수신해야하는 브로커 수를 설정하며 그 값의 브로커가 모두 메시지를 수신해야 쓰기 성공으로 간주한다.
1. 특정 파티션의 리더 리플리카를 포함하는 브로커가 해당 파티션의 쓰기 요청을 받았을 경우 여러가지를 검사한다.
2. 데이터를 전송한 사용자가 해당 토픽의 쓰기 권한을 갖고 있는지, 해당 요청에 지정된 acks 값이 적합한지, 만일 acks가 all로 되어있다면 메시지를 안전하게 쓰는데 충분한 동기화 리플리카들이 있는지를 검사한다. 
3. 그 다음 브로커는 로컬 디스크에 새로 받은 메시지를 쓴다.
4. acks가 0과 1일 경우 즉시 응답을 전송한다. 그렇지 않고 all이면 팔로어 리플리카들이 해당 메시지를 복제했는지 리더가 확인할 때까지 퍼거토리라 하는 버퍼에 해당 요청을 저장한다.

### 카프카 브로커가 읽기 요청을 받았을 때 흐름?
![](https://i.imgur.com/6spvHQG.jpg)
1. 클라이언트가 요청에 지정한 최대, 최소 크기까지의 메시지들을 브로커가 해당 파티션에서 읽은 후 클라이언트에게 전송한다.
> 메시지 크기를 제한하지 않으면 클라이언트의 메모리 부족을 초래할 만큼 큰 응답을 브로커가 전송할 수 있다.
클라이언트는 반환 데이터의 최소 크기도 설정할 수 있다. 이는 CPU와 네트워크 사용을 줄일 수 있는 방법이다. 따라서 브로커는 최소 크기만큼 데이터가 채워지기를 기다렸다가 전송한다. 그렇지만 주어진 시간이 넘어가게 되면 채워지지 않더라도 바로 전송한다.

![](https://i.imgur.com/HxHXZSu.jpg)
2. 카프카는 제로 카피 기법을 사용하여 클라이언트에게 일관성을 위해 모든 ISR에 쓴 메시지들만 전송한다.
> 제로 카피기법은 메시지를 중간 버퍼 메모리에 쓰지 않고 곧바로 네트워크 채널로 전송한다. 이렇게 하면 메모리로부터 데이터를 복사하고 버퍼를 관리하는 부담을 제거하므로 성능이 훨씬 향상된다.
> 만일 동기화 되지 않은 데이터도 읽었을 경우, 동기화되기 전 리더가 죽었을 때 새로운 리더가 선출되는데 다른 컨슈머들은 읽지 않았던 데이터를 읽게 되어 일관성이 결여될 수 있다.

### 카프카 브로커에서 리더와 팔로워가 각자 하는 역할은?
각 파티션은 리더로 지정된 하나의 리플리카를 갖는다. 일관성을 보장하기 위해 모든 프로듀서와 컨슈머의 요청은 리더를 통해서 처리된다. 각 파티션의 리더를 제외한 나머지 리플리카를 팔로어라고 한다. 팔로어는 클라이언트 요청을 서비스 하진 않는다. 대신 리더의 메시지를 복제하여 리더의 것과 동일하게 유지한다.
### 파티션에 대해 리더 팔로워 동기화 과정이 어떻게 되나?
1. 리더는 팔로어 리플리카 중에 어떤 것이 최신 리더 메시지를 복제하고 있는지 알아야한다.
2. 리더와 동기화 하기 위해 리플리카들은 리더에게 fetch 요청을 전송한다. fetch 요청에는 리플리카가 다음으로 받기 원하는 메시지의 오프셋이 포함되며 항상 수신된 순서대로 처리된다.
3. 그럼 요청에 대한 응답으로 리더는 리플리카들에게 메시지를 전송한다. 
> 예를 들면 팔로어 리플리카가 메시지 1 다음 메시지 2 다음 메시지 3을 요청하나 이 메시지를 모두 받기 전까지는 메시지 4를 요청하지 않을 수 있다. 즉 해당 리플리카가 메시지 4를 요청할 때는 메시지 3까지 모두 받았다는 것을 리더가 알 수 있다는 것을 의미한다.
4. 따라서 각 팔로어 리플리카가 요청한 마지막 오프셋을 살펴보면 복제가 얼마나 지연되는지 리더는 알 수 있다. 지연 시간이 지정된 시간을 넘어섰을 경우 리플리카는 ISR(동기화 리플리카)에서 제외된다.
- 기존 리더가 중단되는 경우 동기화 리플리카만이 리더로 선출될 수 있다.
### 리더가 죽었을 경우 신뢰성 있는 환경을 위해 어떻게 해야하는가?
언클린 리더 선출을 false로 하게 되면 죽었던 리더가 살아나더라도 리더 자리를 유지시킨다는 의미이다. 이를 통해 데이터의 유실과 일관성은 유지할 수 있다. 하지만 언제 살아날지 모르는 리더를 하염없이 기다림으로써 가용성이 떨어진다. true로 하게 되면 이전에 follower들이 리더를 완벽하게 동기화하지 못했을 수도 있기에 데이터의 유실과 일관성을 유지하지 못할수도 있으나 가용성은 살릴 수 있다.